{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFoLf79iwja2",
        "outputId": "99dcf6a7-7e19-42b3-a4fd-65c03d410f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-23 15:52:21--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/100/100-0.txt [following]\n",
            "--2024-05-23 15:52:21--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5618815 (5.4M) [text/plain]\n",
            "Saving to: ‘/content/shakespeare.txt’\n",
            "\n",
            "/content/shakespear 100%[===================>]   5.36M  16.5MB/s    in 0.3s    \n",
            "\n",
            "2024-05-23 15:52:22 (16.5 MB/s) - ‘/content/shakespeare.txt’ saved [5618815/5618815]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLrErXbHxkIA",
        "outputId": "82c797d5-51a7-4dbb-e7af-2aaf48cd3457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM\r\n",
            "SHAKESPEARE ***\r\n",
            "﻿The Complete Works of William Shakespeare\r\n",
            "\r\n",
            "by William Shakespeare\r\n",
            "...\n",
            "I’m sorry they offend you, heartily;\n",
            "sound, but not in government.\n",
            "\n",
            "My brother and thy uncle, call’d Antonio—\n",
            "Sir, I have not you by the hand.\n"
          ]
        }
      ],
      "source": [
        "!head -n5 /content/shakespeare.txt\n",
        "!echo \"...\"\n",
        "!shuf -n5 /content/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gg-ek6mEx3zG"
      },
      "outputs": [],
      "source": [
        "# Collecting data and setting methods for pre-processing\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from packaging import version\n",
        "if version.parse(tf.__version__)<version.parse('2.0'):\n",
        "  raise Exception('This notebook is compatible with TensorFlow 2.0 or higer.')\n",
        "\n",
        "SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
        "\n",
        "def transform(txt):\n",
        "  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype =np.int32)\n",
        "\n",
        "def input_fn(seq_len=100, batch_size=1024):\n",
        "  \"\"\"Retrun a dataset of source and target sequences for training.\"\"\"\n",
        "  with tf.io.gfile.GFile(SHAKESPEARE_TXT,'r') as f:\n",
        "    txt = f.read()\n",
        "  source = tf.constant(transform(txt), dtype=tf.int32)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1,drop_remainder =True)\n",
        "\n",
        "  def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "  BUFFER_SIZE = 10000\n",
        "\n",
        "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder = True)\n",
        "\n",
        "  return ds.repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FOF8O17SboYV"
      },
      "outputs": [],
      "source": [
        "# Building the model using LSTM to retain coherency in longer sentences\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "EMBEDDING_DIM = 512\n",
        "DROPOUT_RATE=0.2\n",
        "L2 = 0.00\n",
        "\n",
        "def lstm_model(seq_len=100,batch_size=None, stateful=True):\n",
        "  source = tf.keras.Input(name ='seed', shape=(seq_len,),batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(embedding)\n",
        "  lstm_1 = BatchNormalization()(lstm_1)\n",
        "  lstm_1 = Dropout(DROPOUT_RATE)(lstm_1)\n",
        "\n",
        "  lstm_2 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_1)\n",
        "  lstm_2 = BatchNormalization()(lstm_2)\n",
        "  lstm_2 = Dropout(DROPOUT_RATE)(lstm_2)\n",
        "\n",
        "  lstm_3 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_2)\n",
        "  lstm_3 = BatchNormalization()(lstm_3)\n",
        "  lstm_3 = Dropout(DROPOUT_RATE)(lstm_3)\n",
        "\n",
        "  lstm_4 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_3)\n",
        "  lstm_4 = BatchNormalization()(lstm_4)\n",
        "  lstm_4 = Dropout(DROPOUT_RATE)(lstm_4)\n",
        "\n",
        "  lstm_5 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_4)\n",
        "  lstm_5 = BatchNormalization()(lstm_5)\n",
        "  lstm_5 = Dropout(DROPOUT_RATE)(lstm_5)\n",
        "\n",
        "  lstm_6 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_5)\n",
        "  lstm_6 = BatchNormalization()(lstm_6)\n",
        "  lstm_6 = Dropout(DROPOUT_RATE)(lstm_6)\n",
        "\n",
        "  lstm_7 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_6)\n",
        "  lstm_7 = BatchNormalization()(lstm_7)\n",
        "  lstm_7 = Dropout(DROPOUT_RATE)(lstm_7)\n",
        "\n",
        "  lstm_8 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_7)\n",
        "  lstm_8 = BatchNormalization()(lstm_8)\n",
        "  lstm_8 = Dropout(DROPOUT_RATE)(lstm_8)\n",
        "\n",
        "  lstm_9 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_8)\n",
        "  lstm_9 = BatchNormalization()(lstm_9)\n",
        "  lstm_9 = Dropout(DROPOUT_RATE)(lstm_9)\n",
        "\n",
        "  lstm_10 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_9)\n",
        "  lstm_10 = BatchNormalization()(lstm_10)\n",
        "  lstm_10 = Dropout(DROPOUT_RATE)(lstm_10)\n",
        "\n",
        "  lstm_11 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_10)\n",
        "  lstm_11 = BatchNormalization()(lstm_11)\n",
        "  lstm_11 = Dropout(DROPOUT_RATE)(lstm_11)\n",
        "\n",
        "  lstm_12 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_11)\n",
        "  lstm_12 = BatchNormalization()(lstm_12)\n",
        "  lstm_12 = Dropout(DROPOUT_RATE)(lstm_12)\n",
        "\n",
        "  lstm_13 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_12)\n",
        "  lstm_13 = BatchNormalization()(lstm_13)\n",
        "  lstm_13 = Dropout(DROPOUT_RATE)(lstm_13)\n",
        "\n",
        "  lstm_14 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_13)\n",
        "  lstm_14 = BatchNormalization()(lstm_14)\n",
        "  lstm_14 = Dropout(DROPOUT_RATE)(lstm_14)\n",
        "\n",
        "  lstm_15 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True, kernel_regularizer=l2(L2)))(lstm_14)\n",
        "  lstm_15 = BatchNormalization()(lstm_15)\n",
        "  lstm_15 = Dropout(DROPOUT_RATE)(lstm_15)\n",
        "\n",
        "\n",
        "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256,activation='softmax'))(lstm_15)\n",
        "\n",
        "  return tf.keras.Model(inputs=[source], outputs=[predicted_char])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuqCjm3Vdhot",
        "outputId": "7c11678b-8cdc-4d52-9fe9-8ed48f2e35f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.12.0\n",
            "Running on a TPU w/8 cores\n",
            "TPU system has already been initialized.\n",
            "Epoch 1/50\n",
            "100/100 [==============================] - 102s 425ms/step - loss: 5.4055 - sparse_categorical_accuracy: 0.0926 - lr: 0.0100\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 42s 415ms/step - loss: 2.8237 - sparse_categorical_accuracy: 0.2259 - lr: 0.0100\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 42s 415ms/step - loss: 2.4116 - sparse_categorical_accuracy: 0.2829 - lr: 0.0100\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 41s 415ms/step - loss: 2.0690 - sparse_categorical_accuracy: 0.3556 - lr: 0.0100\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 42s 416ms/step - loss: 1.9671 - sparse_categorical_accuracy: 0.3790 - lr: 0.0100\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 42s 416ms/step - loss: 1.9289 - sparse_categorical_accuracy: 0.3886 - lr: 0.0100\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 42s 416ms/step - loss: 1.9065 - sparse_categorical_accuracy: 0.3952 - lr: 0.0100\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 42s 416ms/step - loss: 1.8905 - sparse_categorical_accuracy: 0.3998 - lr: 0.0100\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 42s 416ms/step - loss: 1.8805 - sparse_categorical_accuracy: 0.4028 - lr: 0.0100\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 42s 418ms/step - loss: 1.8707 - sparse_categorical_accuracy: 0.4060 - lr: 0.0100\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 42s 416ms/step - loss: 1.8615 - sparse_categorical_accuracy: 0.4089 - lr: 0.0100\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 42s 416ms/step - loss: 1.8541 - sparse_categorical_accuracy: 0.4110 - lr: 0.0100\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 41s 415ms/step - loss: 1.8493 - sparse_categorical_accuracy: 0.4128 - lr: 0.0100\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 41s 406ms/step - loss: 2.6966 - sparse_categorical_accuracy: 0.2528 - lr: 0.0100\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.3575 - sparse_categorical_accuracy: 0.3085\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "100/100 [==============================] - 41s 408ms/step - loss: 2.3575 - sparse_categorical_accuracy: 0.3085 - lr: 0.0100\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 53s 528ms/step - loss: 1.9637 - sparse_categorical_accuracy: 0.3870 - lr: 0.0050\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "\n",
        "# BATCH_SIZE = 512\n",
        "# SEQ_LEN = 100\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Set seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define the Reduce learning rate on plateau to make an adapting learning rate to escape the local minimum\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor = 'loss',\n",
        "    factor = 0.5,\n",
        "    patience=2,\n",
        "    min_lr=0.0001,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(monitor='loss',patience=3, restore_best_weights=True)\n",
        "\n",
        "\n",
        "try:\n",
        "  print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "  try:\n",
        "    tf.keras.backend.clear_session()\n",
        "    # try this block if being able to connect to TPU v2\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
        "\n",
        "    # Check if the TPU system has already been initialized\n",
        "    if not tf.config.list_logical_devices('TPU'):\n",
        "      tf.config.experimental_connect_to_cluster(tpu)\n",
        "      tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    else:\n",
        "        print(\"TPU system has already been initialized.\")\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "    with tpu_strategy.scope():\n",
        "      training_model = lstm_model(seq_len=100, stateful=False)\n",
        "      training_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
        "                             loss='sparse_categorical_crossentropy',\n",
        "                             metrics=['sparse_categorical_accuracy'])\n",
        "      training_model.fit(\n",
        "          input_fn(),\n",
        "          steps_per_epoch=100,\n",
        "          epochs=EPOCHS,\n",
        "          callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      training_model.save_weights('/tmp/bard.h5', overwrite=True)\n",
        "  except ValueError:\n",
        "    # raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "    print(\"ERROR: Not connected to a TPU runtime; trying deprecated TPU connection...\")\n",
        "    # if couldn't connect to TPU v2, connect to TPU (deprecated)\n",
        "\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    print('All devices', tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "     # strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "    with strategy.scope():\n",
        "      training_model = lstm_model(seq_len=100, stateful=False)\n",
        "      training_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
        "                            loss='sparse_categorical_crossentropy',\n",
        "                            metrics=['sparse_categorical_accuracy'])\n",
        "      history = training_model.fit(\n",
        "          input_fn(),\n",
        "          steps_per_epoch=100,\n",
        "          epochs=EPOCHS,\n",
        "          callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      training_model.save_weights('/tmp/bard.h5', overwrite=True)\n",
        "\n",
        "# Connecting to CPU/GPU if couldn't connnect to either TPU\n",
        "except Exception as e:\n",
        "  print(f\"TPU connection failed with error: {e}, falling back to CPU/GPU\")\n",
        "  tf.keras.backend.clear_session()\n",
        "  training_model = lstm_model(seq_len=100, stateful=False)\n",
        "  training_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['sparse_categorical_accuracy'])\n",
        "  training_model.fit(\n",
        "      input_fn(),\n",
        "      steps_per_epoch=100,\n",
        "      epochs=EPOCHS,\n",
        "      callbacks=[early_stopping, reduce_lr]\n",
        "  )\n",
        "  training_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uqtrQaqvgNLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c4bff7-a058-4bcd-edf3-031326f987a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            " uouoe u yo, ,ouot y i Ioncd u Io;cn Io.oIlI .ou u uo?oIo?ftcpou ?ouwIosohbh twIbI Y Ion T ucn . uoCouoI yoao;fI I uw?ouo? .o?wuos i .o?oo: iouohok uoao?o.oIcIo; IodoufIoIue.rateinnntngdea?heaegrhae.eveatg,eised-rats.irEnngtsmedegteidngetnnvegenRep\n",
            "\n",
            "PREDICTION 1\n",
            "\n",
            "\n",
            " t n uouosoyl!oaou .oa n df; y soe I uoulu : IcnouoIo,f?o; uoSluo;cuot uououoyfsb,oIbIo,oIog \u001aoyoIoto, J IotoI do; IonoI I .buoIoI ;lI I ! ;oioIou ?cnoIouoIoIl? IoIo? ; .o\n",
            "cI Çouot Ioto;o; ?buoIoaouo. . to.ou !o;oposo?osoIw?ouo?oYodoyo, uwdoIotctw? u\n",
            "\n",
            "PREDICTION 2\n",
            "\n",
            "\n",
            " hotoeou CoIou r , I s ,ou Io;o;cIo?o,o.ono?o?oIo,bIoI tcsceou uo? ,lIly Iod yo?oToI ,lpo; ? I I uoy nl,o;o? u Io: Iouo?ouo!lu soI ybioI ? , uÍenintenisame-ngadeptenn-eesasriexe-i.gnimseshL-dnetnnggsetnsiindisrnd-RsueL--RdnnN-sA.anÀs-\u001ag-pn_iismitgnn\n",
            "\n",
            "PREDICTION 3\n",
            "\n",
            "\n",
            " tos nos Ioy tonotokou go!on e uoy YÍnrgshneUaredeasetiesidnntme.s,aht.-ne.eiesanaeeigdnrthisehd-siptddmesAddnted.n__-ngmas.-ne8sniisdr.e.nRsMAedUvtdnsm-Rti-E\u0017sgsEsagren¶Adde.m.gnMAsUZR._diBMdOMdRpeR-_dsiNsns-Au-sAeg.m_Mg.¾.RiéTMdè.-.S.Eng.R_L.MgM_ng\n",
            "\n",
            "PREDICTION 4\n",
            "\n",
            "\n",
            " not tou so, I noF /o;ou t Ichwuoso?oIoI h ;o, yoy uo;ftou . x dououlno;oI{it.ieiaemniaeevenaiehnatsr..reneiggr-_nmigtgart.ttAig.nsndhg.in.eyn-ZgneRdggsnunintitenrstS.ttggmednmhgtnitSEnjnig.snv-_e-eeAtesðpss¿me-M-nEnZniNi-ndgg-teven..Rshdu-.AgRrghu-\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Predicting values (generating text)\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "# print(prediction_model.shape)\n",
        "\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed,0),BATCH_SIZE, axis=0)\n",
        "\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt)-1):\n",
        "  prediction_model.predict(seed[:,i:i+1], verbose=0)\n",
        "\n",
        "predictions = [seed[:,-1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  last_word = np.array(last_word).reshape((-1,1))\n",
        "  # print(last_word.shape)\n",
        "  next_probits = prediction_model.predict(last_word, verbose=0)[:, 0, :]\n",
        "\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx,dtype=np.int32))\n",
        "\n",
        "for i in range (BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  # generated = ''.join([chr(c) for c in p])\n",
        "  generated = ''.join([chr(int(c)) for c in p])\n",
        "  print(generated)\n",
        "  print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short.'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}