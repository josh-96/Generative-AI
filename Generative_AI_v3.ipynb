{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFoLf79iwja2",
        "outputId": "f8b8df3e-304f-46f0-eb8b-1c400e9db6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-08 11:33:29--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/100/100-0.txt [following]\n",
            "--2024-06-08 11:33:29--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5618815 (5.4M) [text/plain]\n",
            "Saving to: ‘/content/shakespeare.txt’\n",
            "\n",
            "/content/shakespear 100%[===================>]   5.36M  17.3MB/s    in 0.3s    \n",
            "\n",
            "2024-06-08 11:33:29 (17.3 MB/s) - ‘/content/shakespeare.txt’ saved [5618815/5618815]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLrErXbHxkIA",
        "outputId": "6b8529ad-1b5f-431b-90a6-275c3f34cd7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM\r\n",
            "SHAKESPEARE ***\r\n",
            "﻿The Complete Works of William Shakespeare\r\n",
            "\r\n",
            "by William Shakespeare\r\n",
            "...\n",
            "I swear ’tis better to be much abus’d\n",
            "\n",
            "KING JOHN.\n",
            "It is spoke as a Christians ought to speak.\n",
            "Now, Kate, I am a husband for your turn;\n"
          ]
        }
      ],
      "source": [
        "!head -n5 /content/shakespeare.txt\n",
        "!echo \"...\"\n",
        "!shuf -n5 /content/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gg-ek6mEx3zG"
      },
      "outputs": [],
      "source": [
        "# Collecting data and setting methods for pre-processing\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from packaging import version\n",
        "if version.parse(tf.__version__)<version.parse('2.0'):\n",
        "  raise Exception('This notebook is compatible with TensorFlow 2.0 or higer.')\n",
        "\n",
        "SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
        "\n",
        "def transform(txt):\n",
        "  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype =np.int32)\n",
        "\n",
        "def input_fn(seq_len=100, batch_size=1024):\n",
        "  \"\"\"Retrun a dataset of source and target sequences for training.\"\"\"\n",
        "  with tf.io.gfile.GFile(SHAKESPEARE_TXT,'r') as f:\n",
        "    txt = f.read()\n",
        "  source = tf.constant(transform(txt), dtype=tf.int32)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1,drop_remainder =True)\n",
        "\n",
        "  def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "  BUFFER_SIZE = 10000\n",
        "\n",
        "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder = True)\n",
        "\n",
        "  return ds.repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dIYNI7aq32-P"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Attention\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "EMBEDDING_DIM = 512\n",
        "DROPOUT_RATE = 0.3\n",
        "L2 = 0.00\n",
        "\n",
        "def gru_model(seq_len=100, batch_size=None, stateful=True):\n",
        "    source = tf.keras.Input(name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "    embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "    embedding = BatchNormalization()(embedding)\n",
        "\n",
        "    gru = embedding\n",
        "    for i in range(5):  # Using fewer layers to start with\n",
        "        gru = tf.keras.layers.GRU(\n",
        "            EMBEDDING_DIM, stateful=stateful, return_sequences=True,\n",
        "            kernel_regularizer=l2(L2)\n",
        "        )(gru)\n",
        "        gru = BatchNormalization()(gru)\n",
        "        gru = Dropout(DROPOUT_RATE)(gru)\n",
        "\n",
        "    # Adding an attention layer\n",
        "    attention = Attention()([gru, gru])\n",
        "    combined = tf.keras.layers.Concatenate()([gru, attention])\n",
        "\n",
        "    predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(combined)\n",
        "    return tf.keras.Model(inputs=[source], outputs=[predicted_char])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuqCjm3Vdhot",
        "outputId": "6fe3bfae-deaa-4a01-945b-a0871feeb807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.15.0\n",
            "Running on a TPU w/8 cores\n",
            "TPU system has already been initialized.\n",
            "Epoch 1/50\n",
            "100/100 [==============================] - 36s 202ms/step - loss: 3.9142 - sparse_categorical_accuracy: 0.1758 - val_loss: 6.3498 - val_sparse_categorical_accuracy: 0.1509 - lr: 0.0100\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 16s 165ms/step - loss: 2.5034 - sparse_categorical_accuracy: 0.3196 - val_loss: 4.2553 - val_sparse_categorical_accuracy: 0.1255 - lr: 0.0100\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 15s 154ms/step - loss: 2.1540 - sparse_categorical_accuracy: 0.3897 - val_loss: 2.6910 - val_sparse_categorical_accuracy: 0.3239 - lr: 0.0100\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 16s 156ms/step - loss: 2.0569 - sparse_categorical_accuracy: 0.4117 - val_loss: 1.9350 - val_sparse_categorical_accuracy: 0.4319 - lr: 0.0100\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 16s 157ms/step - loss: 1.9325 - sparse_categorical_accuracy: 0.4390 - val_loss: 1.8367 - val_sparse_categorical_accuracy: 0.4720 - lr: 0.0100\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 16s 156ms/step - loss: 1.8640 - sparse_categorical_accuracy: 0.4562 - val_loss: 1.7510 - val_sparse_categorical_accuracy: 0.4823 - lr: 0.0100\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 15s 155ms/step - loss: 1.8102 - sparse_categorical_accuracy: 0.4695 - val_loss: 2.0571 - val_sparse_categorical_accuracy: 0.4305 - lr: 0.0100\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 15s 155ms/step - loss: 1.7649 - sparse_categorical_accuracy: 0.4814 - val_loss: 1.6167 - val_sparse_categorical_accuracy: 0.5169 - lr: 0.0100\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 14s 144ms/step - loss: 1.7280 - sparse_categorical_accuracy: 0.4909 - val_loss: 1.6178 - val_sparse_categorical_accuracy: 0.5221 - lr: 0.0100\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 15s 150ms/step - loss: 1.6942 - sparse_categorical_accuracy: 0.5006 - val_loss: 1.5938 - val_sparse_categorical_accuracy: 0.5258 - lr: 0.0100\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 16s 159ms/step - loss: 1.6654 - sparse_categorical_accuracy: 0.5087 - val_loss: 2.5325 - val_sparse_categorical_accuracy: 0.3162 - lr: 0.0100\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 15s 152ms/step - loss: 1.6431 - sparse_categorical_accuracy: 0.5179 - val_loss: 1.4887 - val_sparse_categorical_accuracy: 0.5610 - lr: 0.0100\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 15s 153ms/step - loss: 1.5885 - sparse_categorical_accuracy: 0.5373 - val_loss: 1.5220 - val_sparse_categorical_accuracy: 0.5575 - lr: 0.0100\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 15s 153ms/step - loss: 1.5511 - sparse_categorical_accuracy: 0.5511 - val_loss: 1.2489 - val_sparse_categorical_accuracy: 0.6261 - lr: 0.0100\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 15s 151ms/step - loss: 1.4339 - sparse_categorical_accuracy: 0.5804 - val_loss: 1.1891 - val_sparse_categorical_accuracy: 0.6401 - lr: 0.0100\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 15s 154ms/step - loss: 1.3828 - sparse_categorical_accuracy: 0.5924 - val_loss: 1.1901 - val_sparse_categorical_accuracy: 0.6443 - lr: 0.0100\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 15s 153ms/step - loss: 1.3508 - sparse_categorical_accuracy: 0.6001 - val_loss: 1.1444 - val_sparse_categorical_accuracy: 0.6489 - lr: 0.0100\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 14s 144ms/step - loss: 1.3457 - sparse_categorical_accuracy: 0.6031 - val_loss: 1.1624 - val_sparse_categorical_accuracy: 0.6444 - lr: 0.0100\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 14s 144ms/step - loss: 1.2945 - sparse_categorical_accuracy: 0.6174 - val_loss: 1.1290 - val_sparse_categorical_accuracy: 0.6638 - lr: 0.0100\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 15s 152ms/step - loss: 1.2603 - sparse_categorical_accuracy: 0.6272 - val_loss: 1.0363 - val_sparse_categorical_accuracy: 0.6825 - lr: 0.0100\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 16s 157ms/step - loss: 1.2262 - sparse_categorical_accuracy: 0.6366 - val_loss: 1.0310 - val_sparse_categorical_accuracy: 0.6817 - lr: 0.0100\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 15s 151ms/step - loss: 1.2033 - sparse_categorical_accuracy: 0.6432 - val_loss: 1.0384 - val_sparse_categorical_accuracy: 0.6861 - lr: 0.0100\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 16s 156ms/step - loss: 1.1802 - sparse_categorical_accuracy: 0.6500 - val_loss: 0.9724 - val_sparse_categorical_accuracy: 0.7013 - lr: 0.0100\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 15s 152ms/step - loss: 1.1553 - sparse_categorical_accuracy: 0.6565 - val_loss: 0.9709 - val_sparse_categorical_accuracy: 0.6986 - lr: 0.0100\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 16s 156ms/step - loss: 1.1369 - sparse_categorical_accuracy: 0.6608 - val_loss: 0.9433 - val_sparse_categorical_accuracy: 0.7068 - lr: 0.0100\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 15s 154ms/step - loss: 1.1274 - sparse_categorical_accuracy: 0.6633 - val_loss: 0.9456 - val_sparse_categorical_accuracy: 0.7071 - lr: 0.0100\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 14s 145ms/step - loss: 1.1081 - sparse_categorical_accuracy: 0.6683 - val_loss: 0.9310 - val_sparse_categorical_accuracy: 0.7109 - lr: 0.0100\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 15s 153ms/step - loss: 1.1010 - sparse_categorical_accuracy: 0.6702 - val_loss: 0.9278 - val_sparse_categorical_accuracy: 0.7108 - lr: 0.0100\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 15s 154ms/step - loss: 1.0935 - sparse_categorical_accuracy: 0.6727 - val_loss: 0.9249 - val_sparse_categorical_accuracy: 0.7099 - lr: 0.0100\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 15s 153ms/step - loss: 1.0857 - sparse_categorical_accuracy: 0.6751 - val_loss: 0.9519 - val_sparse_categorical_accuracy: 0.7049 - lr: 0.0100\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 15s 154ms/step - loss: 1.0780 - sparse_categorical_accuracy: 0.6767 - val_loss: 0.9216 - val_sparse_categorical_accuracy: 0.7145 - lr: 0.0100\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 15s 155ms/step - loss: 1.0698 - sparse_categorical_accuracy: 0.6790 - val_loss: 0.9043 - val_sparse_categorical_accuracy: 0.7182 - lr: 0.0100\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 15s 155ms/step - loss: 1.0579 - sparse_categorical_accuracy: 0.6823 - val_loss: 1.0838 - val_sparse_categorical_accuracy: 0.6896 - lr: 0.0100\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 15s 151ms/step - loss: 1.0689 - sparse_categorical_accuracy: 0.6798 - val_loss: 0.9304 - val_sparse_categorical_accuracy: 0.7112 - lr: 0.0100\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 15s 155ms/step - loss: 1.0467 - sparse_categorical_accuracy: 0.6852 - val_loss: 0.8949 - val_sparse_categorical_accuracy: 0.7219 - lr: 0.0100\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 15s 150ms/step - loss: 1.0394 - sparse_categorical_accuracy: 0.6872 - val_loss: 0.9332 - val_sparse_categorical_accuracy: 0.7144 - lr: 0.0100\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 15s 155ms/step - loss: 1.0292 - sparse_categorical_accuracy: 0.6899 - val_loss: 0.8974 - val_sparse_categorical_accuracy: 0.7202 - lr: 0.0100\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 16s 157ms/step - loss: 1.0260 - sparse_categorical_accuracy: 0.6905 - val_loss: 0.8918 - val_sparse_categorical_accuracy: 0.7233 - lr: 0.0100\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 16s 158ms/step - loss: 1.0198 - sparse_categorical_accuracy: 0.6921 - val_loss: 0.9485 - val_sparse_categorical_accuracy: 0.7112 - lr: 0.0100\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 15s 152ms/step - loss: 1.0152 - sparse_categorical_accuracy: 0.6934 - val_loss: 0.8694 - val_sparse_categorical_accuracy: 0.7317 - lr: 0.0100\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 15s 152ms/step - loss: 1.0091 - sparse_categorical_accuracy: 0.6948 - val_loss: 0.8884 - val_sparse_categorical_accuracy: 0.7278 - lr: 0.0100\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 15s 153ms/step - loss: 1.0018 - sparse_categorical_accuracy: 0.6968 - val_loss: 0.8607 - val_sparse_categorical_accuracy: 0.7346 - lr: 0.0100\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 16s 156ms/step - loss: 0.9999 - sparse_categorical_accuracy: 0.6975 - val_loss: 0.8621 - val_sparse_categorical_accuracy: 0.7326 - lr: 0.0100\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 15s 153ms/step - loss: 0.9872 - sparse_categorical_accuracy: 0.7032 - val_loss: 0.8864 - val_sparse_categorical_accuracy: 0.7279 - lr: 0.0100\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 14s 145ms/step - loss: 0.9714 - sparse_categorical_accuracy: 0.7079 - val_loss: 0.8657 - val_sparse_categorical_accuracy: 0.7327 - lr: 0.0100\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 15s 153ms/step - loss: 0.9624 - sparse_categorical_accuracy: 0.7099 - val_loss: 0.8764 - val_sparse_categorical_accuracy: 0.7359 - lr: 0.0100\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 16s 158ms/step - loss: 0.9527 - sparse_categorical_accuracy: 0.7127 - val_loss: 0.8351 - val_sparse_categorical_accuracy: 0.7422 - lr: 0.0100\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 16s 164ms/step - loss: 0.9447 - sparse_categorical_accuracy: 0.7151 - val_loss: 0.8005 - val_sparse_categorical_accuracy: 0.7552 - lr: 0.0100\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 15s 155ms/step - loss: 0.9303 - sparse_categorical_accuracy: 0.7197 - val_loss: 0.8115 - val_sparse_categorical_accuracy: 0.7530 - lr: 0.0100\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 15s 155ms/step - loss: 0.9179 - sparse_categorical_accuracy: 0.7230 - val_loss: 0.7618 - val_sparse_categorical_accuracy: 0.7635 - lr: 0.0100\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "\n",
        "# BATCH_SIZE = 512\n",
        "# SEQ_LEN = 100\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Set seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define the Reduce learning rate on plateau to make an adapting learning rate to escape the local minimum\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor = 'loss',\n",
        "    factor = 0.5,\n",
        "    patience=2,\n",
        "    min_lr=0.0001,\n",
        "    verbose=1\n",
        ")\n",
        "early_stopping = EarlyStopping(monitor='loss',patience=3, restore_best_weights=True)\n",
        "\n",
        "\n",
        "try:\n",
        "  print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "  try:\n",
        "    tf.keras.backend.clear_session()\n",
        "    # try this block if being able to connect to TPU v2\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
        "\n",
        "    # Check if the TPU system has already been initialized\n",
        "    if not tf.config.list_logical_devices('TPU'):\n",
        "      tf.config.experimental_connect_to_cluster(tpu)\n",
        "      tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    else:\n",
        "        print(\"TPU system has already been initialized.\")\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "    with tpu_strategy.scope():\n",
        "      training_model = gru_model(seq_len=100, stateful=False)\n",
        "      training_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
        "                             loss='sparse_categorical_crossentropy',\n",
        "                             metrics=['sparse_categorical_accuracy'])\n",
        "      training_model.fit(\n",
        "          input_fn(),\n",
        "          steps_per_epoch=100,\n",
        "          validation_data=input_fn(),\n",
        "          validation_steps=10,\n",
        "          epochs=EPOCHS,\n",
        "          callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      training_model.save_weights('/tmp/bard.h5', overwrite=True)\n",
        "  except ValueError:\n",
        "    # raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "    print(\"ERROR: Not connected to a TPU runtime; trying deprecated TPU connection...\")\n",
        "    # if couldn't connect to TPU v2, connect to TPU (deprecated)\n",
        "\n",
        "\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    print('All devices', tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "     # strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "    with strategy.scope():\n",
        "      training_model = gru_model(seq_len=100, stateful=False)\n",
        "      training_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
        "                            loss='sparse_categorical_crossentropy',\n",
        "                            metrics=['sparse_categorical_accuracy'])\n",
        "      history = training_model.fit(\n",
        "          input_fn(),\n",
        "          steps_per_epoch=100,\n",
        "          validation_data=input_fn(),\n",
        "          validation_steps=10,\n",
        "          epochs=EPOCHS,\n",
        "          callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "      training_model.save_weights('/tmp/bard.h5', overwrite=True)\n",
        "\n",
        "# Connecting to CPU/GPU if couldn't connnect to either TPU\n",
        "except Exception as e:\n",
        "  print(f\"TPU connection failed with error: {e}, falling back to CPU/GPU\")\n",
        "  tf.keras.backend.clear_session()\n",
        "  training_model = gru_model(seq_len=100, stateful=False)\n",
        "  training_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['sparse_categorical_accuracy'])\n",
        "  training_model.fit(\n",
        "          input_fn(),\n",
        "          steps_per_epoch=100,\n",
        "          validation_data=input_fn(),\n",
        "          validation_steps=10,\n",
        "          epochs=EPOCHS,\n",
        "          callbacks=[early_stopping, reduce_lr]\n",
        "      )\n",
        "  training_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqtrQaqvgNLZ",
        "outputId": "7f161b2f-9fd5-48a8-dd30-03099aeaab4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            " Richard in ùastand.\r\n",
            "Their _ad for smild chasted not       606\r\n",
            "\r\n",
            "KING ÑIIBAN.\r\n",
            "As have                      942\r\n",
            "¯arge een quality by Xenwick,\r\n",
            "What he oments name but, sir\r\n",
            "INGER III. Norsoner falseing ¢alice 9SHON IISHAM.\r\n",
            "~ow enter OctaviisBan\n",
            "PREDICTION 1\n",
            "\n",
            "\n",
            " Ill  to\r\n",
            " INGHAM AND IV ANDRANW OF SYRACUSE.\r\n",
            "The Æhe lay we do me,\r\n",
            "And of one ´ON HENRY.\r\n",
            "Now enter them      Somerset like !ingd well they Áen.\r\n",
            "Shall quarrel this by Hesmen Caesars \u001bales üIBIX  BLEUT\r\n",
            "HAMLET AND FARTHER.\r\n",
            "O, and a ambition my ma\n",
            "PREDICTION 2\n",
            "\n",
            "\n",
            " Look and Eather guard æaly free prepare\r\n",
            "The ing }erberance Claudio in  princess by the issue to dead Netrack\r\n",
            "Let us entistance and I ïPHESUS FRYER.\r\n",
            " stand so hardRhinked before have us any noble 9man your ¢ing:\r\n",
            "In be part engable be(\r\n",
            "Makes th\n",
            "PREDICTION 3\n",
            "\n",
            "\n",
            " you sand of 2enà\r\n",
            "This once had not Ill dimn did\r\n",
            "   And he ûhat the ¢ales Âd now Temptì\r\n",
            "\r\n",
            "SECOND OF SYRACUSE.\r\n",
            "To infect notwith of favours arcellence.\r\n",
            "Have every \u001eoljerly have pay not forgive\r\n",
            "And enedones in his congrained:\r\n",
            "The kith fly I of\n",
            "PREDICTION 4\n",
            "\n",
            "\n",
            " \u0015hall against not for heaven pleased!\r\n",
            "Èing \u001aales forth ever -escamd:\r\n",
            "(Shall have the thunder of\n",
            "have\r\n",
            "Scene 8IMIAN KING HENRY.\r\n",
            "[_Cometh._]\r\n",
            "Change À virtuous names Ìhomachs ¬y that revenge best not add a faireer goodDy\r\n",
            "óINTTINE OF confusd,\r\n",
            "zALI\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-8d2851abf283>:35: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  generated = ''.join([chr(int(c)) for c in p])\n"
          ]
        }
      ],
      "source": [
        "# Predicting values (generating text)\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "prediction_model = gru_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "# print(prediction_model.shape)\n",
        "\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed,0),BATCH_SIZE, axis=0)\n",
        "\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt)-1):\n",
        "  prediction_model.predict(seed[:,i:i+1], verbose=0)\n",
        "\n",
        "predictions = [seed[:,-1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  last_word = np.array(last_word).reshape((-1,1))\n",
        "  # print(last_word.shape)\n",
        "  next_probits = prediction_model.predict(last_word, verbose=0)[:, 0, :]\n",
        "\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx,dtype=np.int32))\n",
        "\n",
        "for i in range (BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  # generated = ''.join([chr(c) for c in p])\n",
        "  generated = ''.join([chr(int(c)) for c in p])\n",
        "  print(generated)\n",
        "  # print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short.'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
