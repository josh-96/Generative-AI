{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFoLf79iwja2",
        "outputId": "4d546940-03c8-4252-9e38-7a5bc2c68934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-21 12:16:20--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/100/100-0.txt [following]\n",
            "--2024-05-21 12:16:21--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLrErXbHxkIA",
        "outputId": "109ce6cb-9729-4798-d106-7b934c937243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM\r\n",
            "SHAKESPEARE ***\r\n",
            "﻿The Complete Works of William Shakespeare\r\n",
            "\r\n",
            "by William Shakespeare\r\n",
            "...\n",
            "My heart prays for him, though my tongue do curse.\n",
            "Remembering how I love thy company.\n",
            "\n",
            "breath?\n",
            "Well, sir, we must have you find your legs.\n"
          ]
        }
      ],
      "source": [
        "!head -n5 /content/shakespeare.txt\n",
        "!echo \"...\"\n",
        "!shuf -n5 /content/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gg-ek6mEx3zG"
      },
      "outputs": [],
      "source": [
        "# Collecting data and setting methods for pre-processing\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from packaging import version\n",
        "if version.parse(tf.__version__)<version.parse('2.0'):\n",
        "  raise Exception('This notebook is compatible with TensorFlow 2.0 or higer.')\n",
        "\n",
        "SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
        "\n",
        "def transform(txt):\n",
        "  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype =np.int32)\n",
        "\n",
        "def input_fn(seq_len=100, batch_size=1024):\n",
        "  \"\"\"Retrun a dataset of source and target sequences for training.\"\"\"\n",
        "  with tf.io.gfile.GFile(SHAKESPEARE_TXT,'r') as f:\n",
        "    txt = f.read()\n",
        "  source = tf.constant(transform(txt), dtype=tf.int32)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1,drop_remainder =True)\n",
        "\n",
        "  def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "  BUFFER_SIZE = 10000\n",
        "\n",
        "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder = True)\n",
        "\n",
        "  return ds.repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FOF8O17SboYV"
      },
      "outputs": [],
      "source": [
        "# Building the model using LSTM to retain coherency in longer sentences\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
        "\n",
        "EMBEDDING_DIM = 512\n",
        "DROPOUT_RATE=0.3\n",
        "\n",
        "def lstm_model(seq_len=100,batch_size=None, stateful=True):\n",
        "  source = tf.keras.Input(name ='seed', shape=(seq_len,),batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(embedding)\n",
        "  lstm_1 = BatchNormalization()(lstm_1)\n",
        "  lstm_1 = Dropout(DROPOUT_RATE)(lstm_1)\n",
        "\n",
        "  lstm_2 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(lstm_1)\n",
        "  lstm_2 = BatchNormalization()(lstm_2)\n",
        "  lstm_2 = Dropout(DROPOUT_RATE)(lstm_2)\n",
        "\n",
        "  lstm_3 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(lstm_2)\n",
        "  lstm_3 = BatchNormalization()(lstm_3)\n",
        "  lstm_3 = Dropout(DROPOUT_RATE)(lstm_3)\n",
        "\n",
        "  lstm_4 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(lstm_3)\n",
        "  lstm_4 = BatchNormalization()(lstm_4)\n",
        "  lstm_4 = Dropout(DROPOUT_RATE)(lstm_4)\n",
        "\n",
        "  lstm_5 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(lstm_4)\n",
        "  lstm_5 = BatchNormalization()(lstm_5)\n",
        "  lstm_5 = Dropout(DROPOUT_RATE)(lstm_5)\n",
        "\n",
        "  lstm_6 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(lstm_5)\n",
        "  lstm_6 = BatchNormalization()(lstm_6)\n",
        "  lstm_6 = Dropout(DROPOUT_RATE)(lstm_6)\n",
        "\n",
        "  lstm_7 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(lstm_6)\n",
        "  lstm_7 = BatchNormalization()(lstm_7)\n",
        "  lstm_7 = Dropout(DROPOUT_RATE)(lstm_7)\n",
        "\n",
        "  '''commented out due to power requirements. It uses too much computational power if not using TPU, otherwise, works well.'''\n",
        "  # lstm_8 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(lstm_7)\n",
        "  # lstm_8 = BatchNormalization()(lstm_8)\n",
        "  # lstm_8 = Dropout(DROPOUT_RATE)(lstm_8)\n",
        "\n",
        "  # lstm_9 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(lstm_8)\n",
        "  # lstm_9 = BatchNormalization()(lstm_9)\n",
        "  # lstm_9 = Dropout(DROPOUT_RATE)(lstm_9)\n",
        "\n",
        "  # lstm_10 = (tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True))(lstm_9)\n",
        "  # lstm_10 = BatchNormalization()(lstm_10)\n",
        "  # lstm_10 = Dropout(DROPOUT_RATE)(lstm_10)\n",
        "\n",
        "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256,activation='softmax'))(lstm_7)\n",
        "\n",
        "  return tf.keras.Model(inputs=[source], outputs=[predicted_char])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuqCjm3Vdhot",
        "outputId": "e8ab402c-0675-47de-f4f9-c0a111f71b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 377s 4s/step - loss: 3.7209 - sparse_categorical_accuracy: 0.1538\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 359s 4s/step - loss: 2.6724 - sparse_categorical_accuracy: 0.2406\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 360s 4s/step - loss: 2.5000 - sparse_categorical_accuracy: 0.2768\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 360s 4s/step - loss: 2.3687 - sparse_categorical_accuracy: 0.3076\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 361s 4s/step - loss: 2.3085 - sparse_categorical_accuracy: 0.3235\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 360s 4s/step - loss: 2.2164 - sparse_categorical_accuracy: 0.3465\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 361s 4s/step - loss: 2.1104 - sparse_categorical_accuracy: 0.3741\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 361s 4s/step - loss: 1.9975 - sparse_categorical_accuracy: 0.4041\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 361s 4s/step - loss: 1.9163 - sparse_categorical_accuracy: 0.4259\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 362s 4s/step - loss: 1.8543 - sparse_categorical_accuracy: 0.4426\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss',patience=3, restore_best_weights=True)\n",
        "\n",
        "try:\n",
        "  tf.keras.backend.clear_session()\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "  tf.config.experimental_conect_to_cluster(resolver)\n",
        "\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  print('All devices', tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "  with strategy.scope():\n",
        "    training_model = lstm_model(seq_len=100, stateful=True)\n",
        "    training_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
        "                          loss='sparse_categorical_crossentropy',\n",
        "                          metrics=['sparse_categorical_accuracy'])\n",
        "    training_model.fit(\n",
        "        input_fn(),\n",
        "        steps_per_epoch=100,\n",
        "        epochs=10\n",
        "    )\n",
        "    training_model.save_weights('tmp/bard.h5', overwrite=True)\n",
        "except:\n",
        "  training_model = lstm_model(seq_len=100, stateful=False)\n",
        "  training_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
        "                         loss='sparse_categorical_crossentropy',\n",
        "                         metrics=['sparse_categorical_accuracy'])\n",
        "  training_model.fit(\n",
        "      input_fn(),\n",
        "      steps_per_epoch=100,\n",
        "      epochs=10,\n",
        "      callbacks=[early_stopping]\n",
        "  )\n",
        "  training_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uqtrQaqvgNLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b050b1c-0232-4879-cc2d-91045bfa4995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            " Gress shere, batter for not?\r\n",
            "for shy, hear firt she tell her pill fall answee wervary hreatfer-take saftor dear. I this mut nimgenss respare him graise.\r\n",
            "Camrece her time ear wombon mut deed on the cheece hear sorth house dobe hus the desench to fa\n",
            "\n",
            "PREDICTION 1\n",
            "\n",
            "\n",
            " And have, but be clet Caunst doth not come is you,\r\n",
            "The breets un faite anotheres did. O heart perker?\r\n",
            "\r\n",
            "DALVOLIA.\r\n",
            "Come in Nevery seem, a nutt troke three!\r\n",
            "I she not grom into his the depar not vort his deps, grace to dist him\r\n",
            "Ajen sliph the sin\n",
            "\n",
            "PREDICTION 2\n",
            "\n",
            "\n",
            " That to prodc?\r\n",
            "\r\n",
            "PURTIES.\r\n",
            "I the arrow the know.\r\n",
            "\r\n",
            "mOLENNA.\r\n",
            "Fire may you ro sith Prove and the service most year woflocion.\r\n",
            "\r\n",
            " A_LIALO.\r\n",
            "Nes mreet her servan god Mesporm, no; bo wo. the ever: and I am was so has is sith him me be\r\n",
            "thy demtire an\n",
            "\n",
            "PREDICTION 3\n",
            "\n",
            "\n",
            " Loint mut the head adm?\r\n",
            "The honours caur not muscoul\r\n",
            "Ne you dusines, chall not not less, shile thy are I haster\r\n",
            "art afpq faes is hath the greath that Dosn cart sork, her tume sonspar you dadk staters jind a sHirt. Shich tree on the leve me that i\n",
            "\n",
            "PREDICTION 4\n",
            "\n",
            "\n",
            " Darking destriend\r\n",
            "Liof for his the bloasent deaph, not thee that looked like not vies; her dotaly win of leace; or his sill rest?\r\n",
            "[_Tven Bit plaid mut Jest his with rughtiance. Thou mave you. Bot with anothin.\r\n",
            "It jnepillened thou jnow to Athery s\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-02555af958fb>:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  generated = ''.join([chr(int(c)) for c in p])\n"
          ]
        }
      ],
      "source": [
        "# Predicting values (generating text)\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "# print(prediction_model.shape)\n",
        "\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed,0),BATCH_SIZE, axis=0)\n",
        "\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt)-1):\n",
        "  prediction_model.predict(seed[:,i:i+1], verbose=0)\n",
        "\n",
        "predictions = [seed[:,-1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  last_word = np.array(last_word).reshape((-1,1))\n",
        "  # print(last_word.shape)\n",
        "  next_probits = prediction_model.predict(last_word, verbose=0)[:, 0, :]\n",
        "\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx,dtype=np.int32))\n",
        "\n",
        "for i in range (BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  # generated = ''.join([chr(c) for c in p])\n",
        "  generated = ''.join([chr(int(c)) for c in p])\n",
        "  print(generated)\n",
        "  print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short.'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}